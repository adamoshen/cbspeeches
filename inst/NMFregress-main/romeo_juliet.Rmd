---
title: "A Brief Tutorial on NMFregress"
author: Gabriel Phelan & Dave Campbell
date: July 2024
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document will serve as a brief introduction to the `NMFregress` package, which smashes together a host of ideas surrounding non-negative matrix factorization (NMF) and non-parametric inference. The point is to solve some problems that are typically addressed using complicated probabilistic graphical models. There's nothing wrong with graphical models; in fact, they're quite cool! The issue is that fitting them can be a major pain; entire careers have been built around inference algorithms for graphical models. Despite the rapid progress in this field -- think HMC and its implementation in the wonderful Stan package -- certain model classes have yet to be adorned with inference schemes whose speed, ease of use, and statistical accuracy are fully convincing. For this reason we take a different tack. The basic idea is to slap permutation tests and the boostrap on top of some recent developments in NMF research. Motivation is provided by topic models in which there is some sort of discrete covariate over the documents -- we might partition news stories by their source or a collection of books by author, for instance. This is the setting we'll stick to in what follows, but know that `NMFregress` can be used for good old NMF, whatever your application domain might be. The package's langauge is unabashedly slanted towards topic models, but you can simply ignore that and get on with factoring matrices if you wish; for example, a "term-document matrix" could really be any non-negative data matrix. Please refer to [link to thesis when available] for further details on the motivation and underlying mathematics, as well as references to the literature. 

## 1. Building a Corpus

We want to do topic modelling, so we'll need some documents. Let's use Romeo and Juliet (RJ), freely available on Project Gutenberg: https://www.gutenberg.org/ebooks/1112. Lines in the play will be considered documents, and we'll partition them by the act in which they appear. Since acts are deliberately chosen to divide a play's plot into coherent segments, we should be able to detect statistical variation in Shakespeare's language as the play progresses. I've prepared a term-document matrix (TDM) and a factor containing the play's acts using an admittedly haphazard script contained in this repository -- it's not the prettiest but it gets the job done. Let's load the library and the RJ data: 



```{r, message = FALSE}
library(tidyverse)
library(ggpubr) # for arranging plots 

```



```{r, message = FALSE, results='hide'}
# It's clunky for for version 0.5 just source the files after cloning the git repo
anchor_files = list.files("../NMFregress", pattern = ".R$")
lapply(paste0("../NMFregress/",anchor_files), source)

# make the datasets available
load(paste0("../NMFregress/data/tdm.Rdata"))
load(paste0("../NMFregress/data/words.Rdata"))
load(paste0("../NMFregress/data/acts.Rdata"))
```


## 2. Initializing an Object for NMF

The first step is to build an object that can be passed to the NMF solver. Rather than the solver requiring a slew of arguments, we instead create an object of class `nmf_input` which carries around the necesarry pieces, specified once and for all by the user. Most importantly, the user must create and pass a term-document matrix, which is a good ol' R matrix -- not a term-document matrix as created using some other topic modelling packages. You're on your own here -- we provide no functionality for creating a TDM or any of the other pre-processing that topic modelling usually entails. Also of note: we really mean TDM, and not DTM! Make sure the words are rows, otherwise everything will come out backwards! 
 
 
Once you've got your TDM, you need some discrete covariate structure over the documents -- a partitioning, as we'll refer to it. These are the acts of RJ in our example. The first 217 documents (which recall are lines in the play) belong to act one, for instance. This is encoded as a factor of equal length to the number of documents, with the entries giving the value of the covariate like so:

```{r}
acts = factor(acts, levels = c("one","two","three","four","five"))
head(acts)

```
To make use of the covariates we need something that will act like a design matrix.  Let's convert _acts_ into a dummy variable.  Let's also make a numeric version of acts so that we can test our additional functionality. 

```{r}
acts_numeric = acts|> case_match(
  "one" ~ 1,
  "two" ~ 2,
  "three" ~ 3,
  "four" ~ 4,
  "five" ~ 5
)
acts_numeric = data.frame(intercept = 1, act = acts_numeric )

acts_dummy = model.matrix(~.,data.frame(acts))
```




Finally, we'll need a character vector which contains all the unique words in the corpus. Crucially, they have to be in the same order as that of the TDM's rows. Again, you're on your own here -- create this vector however you see fit. Here's the first 25 words in our RJ courpus:

```{r}
head(words, n = 25)
```


That's it! Now simply run 

```{r}

my_input = create_input(tdm, vocab = words, covariates = acts_dummy, topics = 25)
my_input|> names()
```

which creates an object `my_input` of class `nmf_input` (this is basically just a named list); the last argument tells the solver we want 25 topics in our topic model. 

## 3. Solving NMF 

Since `my_input` contains (nearly) all the information the solver needs, we now simply put 

```{r, results="hide"}
my_output = solve_nmf(my_input)
```

And there you have it. `my_output` contains $\Phi$ (the word-topic matrix), $\Theta$ (the topic-document matrix), the anchor words, and some relevant infomration from `my_input` that the inference functions will need to do their thing:

```{r}
str(my_output)
```

If you're just interested in the usual topic modelling pipeline, then you're done at this stage. Print the top words for each topic using 

```{r}
print_top_words(my_output, n = 10)
``` 

where `n=10` says we'd like 10 words printed per topic. Topics are labelled by their anchor word. 

Now, a word on the optional argument in  `create_input(..., project = FALSE)`. This tells the solver that we do *not* want to perform random projections prior to solving NMF. Random projections send the data down to a random low-dimensional subspace, perform the linear algebra calculations there, and then map back up to the original space. Used appropriately, these can get provably close to the original solution while providing potentially large speed-ups. The TDM is small enough here where this isn't really an issue, and in fact such a projection may even be detrimental since "small" data is subject to higher distorition. But, for large matrices this is a really nice option; we'd specify something like 

```{r,  eval = FALSE}
my_input2 = create_input(tdm, words, covariates = acts_dummy, topics = 25 , project = TRUE, proj_dim = 200)
my_output2 = solve_nmf(my_input2)
```

which tells the solver we'd like to project down to 200 dimensions to find the anchor words and likewise for solving the non-negative least squares problems that constitute convex NMF. It's up to you to get these dimensions right -- that is, ensuring that they are actually smaller than the input dimension (which is the number of columns in the TDM). For some context, we've solved NMF for matrices of size ~ 10,000 x 6,000 in a matter of minutes with very good results. Again, this is down to the data as well -- the more signal your TDM carries, the better these projections will work. 


## Normalization

Documents have different lengths and the user may see fit to handle this in one of a variety of ways depending on their goals and the domain of interest.  As with all pre-processing, choices have downstream impacts and results are conditional on these choices.  Normalization is the process of scaling the document decomposition in some way so as to avoid mundane inference that is to be interpreted as covariate X is associated with longer documents.  The preferred normalization method is to normalize the $\theta$ matrix so that the topic weights for a document sum to 1.  Normalizing $\theta$ provides the interpretation as the probability of topics for a document.  The bounded elements of $\theta \in [0,1]$, imply that a beta regression is appropriate.  However since topic allocations depend on the across-topic-sums for a document, the scaling will depend on the number of topics.  Inference for this model targets relative effect sizes conditional on the number of topics.  Topics are nearly orthogonal so typically changing the number of topics has little practical impact



## 4. Inferece I. OLS and bootstrap

The interpretation of performing regression on something akin to a probabilistic topic model implies that OLS will probably break the Gauss Markov theorem conditions.  However it is reasonable to use under some conditions.  Linear models are appropriate here if we set act as a factor and we use bootstrap for interval estimates.

 Recall that these are the play's acts. Basically, we're interested in the relationship between topics and groups. For this we run:

```{r, message = FALSE, warning=FALSE, results = FALSE }
OLS_bootstrap = boot_reg(my_output, samples = 1000, Model = "OLS")

# Test the input of a subset of topics
```
A formula could be specified.  By default a formula is derived from all the columns of the covariate matrix.  Note that an intercept must be provided  as a column of the covariate matrix if you go this default route.

What we get is the bootstrap samples for the regression model of $text_i = \beta_0+ \sum_{act = 1}^4 X_{i,act} \beta_{act} + error_i$.

Let's extract and plot the effect sizes for the regression coefficients for the topics of **love** and **night**.
Note that we ran a model where act 1 was the baseline, so if all other effects are positive or negative relative to the baseline.  I am rounding the error bars, for viewing convenience. 


```{r, message = FALSE}
# plot the raw OLS coefficients:

ols_boot_juliet = boot_plot(OLS_bootstrap, "juliet") 
ols_boot_dead   = boot_plot(OLS_bootstrap, "dead")
ggarrange(ols_boot_juliet, ols_boot_dead, 
          ncol = 1, nrow = 2)

# or plot the model fit by including new data at which to evaluate the OLS model:
newdata = my_output$covariates|> unique()
rownames(newdata) = c("act1","act2","act3","act4","act5")

ols_fit_juliet = boot_plot(OLS_bootstrap, "juliet", newdata) 
ols_fit_dead   = boot_plot(OLS_bootstrap, "dead", newdata)
ggarrange(ols_fit_juliet, ols_fit_dead, 
          ncol = 1, nrow = 2)

create_error_bars(OLS_bootstrap, topic = c("dead"))|>
    dplyr::mutate(
      dplyr::across(where(is.numeric), ~round(.,digits =4))
      )

```

## 5. Inference II.  Beta regression and Asymptotic inference.

Beta regression is useful when the data is bounded (0,1) and we wish to model covariates which are factors of continuous.  Here we just have **acts** to model, so for the purposes of exposition let's assume that the topic of interest, **dead** may be building up as the play moves along.

Note that Beta regression is much slower than OLS.  We could still use bootstrap, but be prepared to wait orders of magnitude longer. Instead we set **samples=1** and **return_just_coefs=FALSE**.  This runs beta regression from the **betareg** package on each selected topic.  The entire output of the **betareg::betareg** is returned for each topic selected.  There is substantial flexibility for setting up models for the mean and precision.  Here by default we use the same model for each based on the full set of coefficients and the default logit link for the mean and a log link for precision.

First we swap out the covariates from the setup so that we run the model of interest. Then we select the topics of interest.  Here we will use:
\{"night","love","juliet", "death", "dead"\}.

```{r, message = FALSE, warning=FALSE, results = FALSE}
# my_output$covariates = acts_numeric
my_output$covariates = acts_dummy
# just the model without bootstrap
beta_asymptotic = get_regression_coefs(my_output, 
                           Model = "BETA", 
                           return_just_coefs = FALSE,
                           topics = c("night","love","juliet", "death", "dead"))

# to do full bootstrap:
beta_boot = boot_reg(my_output, 
                           samples = 100, Model = "BETA", 
                           return_just_coefs = TRUE,
                           topics = c("night","love","juliet", "death", "dead"))


beta_asymptotic_juliet = boot_plot(beta_asymptotic, "juliet")
beta_asymptotic_dead   = boot_plot(beta_asymptotic, "dead")
ggarrange(beta_asymptotic_juliet, beta_asymptotic_dead, 
          ncol = 1, nrow = 2)
# or include new observations for the model:
newdata = my_output$covariates|> unique()
rownames(newdata) = c("act1","act2","act3","act4","act5")

beta_asymptotic_juliet_newdata = boot_plot(beta_asymptotic, "juliet", newdata = newdata)
beta_asymptotic_dead_newdata   = boot_plot(beta_asymptotic, "dead"  , newdata = newdata)
ggarrange(beta_asymptotic_juliet_newdata, beta_asymptotic_dead_newdata, 
          ncol = 1, nrow = 2)


beta_boot_juliet = boot_plot(beta_boot, "juliet")
beta_boot_dead   = boot_plot(beta_boot, "dead")
ggarrange(beta_asymptotic_juliet_newdata, beta_boot_juliet,
          beta_asymptotic_dead_newdata, beta_boot_dead,
          ncol = 2, nrow = 2)

# or include new observations for the model:
beta_boot_juliet_newdata = boot_plot(beta_boot, "juliet",newdata)
beta_boot_dead_newdata   = boot_plot(beta_boot, "dead",newdata)

ggarrange(beta_asymptotic_juliet_newdata, beta_boot_juliet,beta_boot_juliet_newdata,
          beta_asymptotic_dead_newdata, beta_boot_dead,beta_boot_dead_newdata,
          ncol = 3, nrow = 2)
```



Model output a list of length = the number of samples.  Since we have **samples =1** then we need to extract the model output from the first element of the list.  Then we can call by topic and use the asymptotic results for the beta regression.


```{r}
beta_asymptotic$death|> summary()
```

### Note about width of beta regression intervals:

Note that when running beta regression on a small number of documents will likely include _long tails_.  Here it shows up as the .025 and .5 quantiles being close to zero, but the .975 quantile is close to 1.  With more documents, there is more information that will tighten the density around the point estimate.

## 6. Inference III.  Generalized Additive Model with Beta Regression

This comes from the **mgcv** package using  _beta_ regression.  These intervals are additive and can produce values beyond the [0,1] bounds.  The way around that is more complicated and beyond this software version.  


```{r, message = FALSE, warning=FALSE, results = FALSE }
# Need to fake spread out the covariates so that a GAM might be reasonable
my_output$covariates = matrix(acts_numeric[,2]+rnorm(n = length(acts_numeric[,2]), mean = 0, sd = 1),ncol = 1)
colnames(my_output$covariates) = "acts"
beta_gam = get_regression_coefs(output = my_output, 
                    Model =  "GAM", 
                    return_just_coefs = FALSE,
                    topics = c("night","love","juliet", "death", "dead")
         )
boot_plot(beta_gam, 
          topic = "death", 
          newdata = data.frame(acts=1:5))


```


## Finally

So `juliet`, `death`, are increasingly likely as the story moves along.

